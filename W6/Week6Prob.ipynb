{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0844afb4",
   "metadata": {},
   "source": [
    "# Assignment Goals\n",
    "\n",
    "Assignment 6 requires you to implement gradient descent based optimization.  \n",
    "\n",
    "- Minimum requirement: adapt the code from the presentation to optimize as many of the functions below as possible.\n",
    "- Write a generic function that will take in 2 other functions as input, and a range of values within which to search, and then implement gradient descent to find the optimum.  The basic requirements of gradient descent are already available in the presentation.\n",
    "-  For some assignments, the gradient has not been given.  You can either write the function on your own, or suggest other methods that can achieve this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e669c869-9fd2-4428-985d-0e8583613c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following imports are assumed for the rest of the problems\n",
    "import numpy as np\n",
    "from numpy import cos, sin, pi, exp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffc3cc",
   "metadata": {},
   "source": [
    "## Problem 1 - 1-D simple polynomial\n",
    "\n",
    "The gradient is not specified.  You can write the function for gradient on your own.  The range within which to search for minimum is [-5, 5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5df21fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.750000000010272"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return x ** 2 + 3 * x + 8\n",
    "\n",
    "def f2(x):\n",
    "    return 2*x + 3\n",
    "\n",
    "def gradient_descent_for_1var(func, derivative, start_point_range, learning_rate, precesion=1e-6, max_iterations=10000):\n",
    "    st = np.linspace(start_point_range[0],start_point_range[1],100)\n",
    "    function_values = []\n",
    "    point_values = []\n",
    "    for start_point in st:\n",
    "        current_point = start_point\n",
    "        for i in range(max_iterations):\n",
    "            prev_point = current_point\n",
    "            gradient = derivative(current_point)\n",
    "            current_point -= learning_rate * gradient\n",
    "            if abs(current_point - prev_point) < precesion:\n",
    "                break\n",
    "        point_values.append(current_point)\n",
    "        function_values.append(func(current_point))\n",
    "    return min(function_values)\n",
    "        \n",
    "    \n",
    "gradient_descent_for_1var(f1,f2,[-5,5],0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f81aa8",
   "metadata": {},
   "source": [
    "## Problem 2 - 2-D polynomial\n",
    "\n",
    "Functions for derivatives, as well as the range of values within which to search for the minimum, are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6535029a-caea-4da4-a613-fb6a2d69491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.00001072887693"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlim3 =  [-10, 10]\n",
    "ylim3 =  [-10, 10]\n",
    "def f3(x, y):\n",
    "    return x**4 - 16*x**3 + 96*x**2 - 256*x + y**2 - 4*y + 262\n",
    "\n",
    "def df3_dx(x, y):\n",
    "    return 4*x**3 - 48*x**2 + 192*x - 256\n",
    "\n",
    "def df3_dy(x, y):\n",
    "    return 2*y - 4\n",
    "\n",
    "def pythagores(x1,y1,x2,y2):\n",
    "    distance = np.sqrt((x1-x2)**2 + (y1-y2)**2)\n",
    "    return distance\n",
    "\n",
    "\n",
    "def gradient_descent_for_2variables(func, derivative_x,derivative_y,start_point_x_range,start_point_y_range, learning_rate, precesion=1e-6, max_iterations=10000):\n",
    "    st_x = np.linspace(start_point_x_range[0],start_point_x_range[1],100)\n",
    "    st_y = np.linspace(start_point_y_range[0],start_point_y_range[1],100)\n",
    "    \n",
    "    x_vals,y_vals,z_vals = [],[],[]\n",
    "    for start_point_x,start_point_y in zip(st_x,st_y):\n",
    "    \n",
    "        current_point_x,current_point_y = start_point_x,start_point_y\n",
    "        for i in range(max_iterations):\n",
    "            prev_point_x,prev_point_y = current_point_x,current_point_y\n",
    "            gradient_x,gradient_y = derivative_x(current_point_x,prev_point_y),derivative_y(prev_point_x,current_point_y)\n",
    "\n",
    "            current_point_x -= learning_rate * gradient_x\n",
    "            current_point_y -= learning_rate * gradient_y\n",
    "\n",
    "            if (pythagores(current_point_x,current_point_y,prev_point_x,prev_point_y)) < precesion:\n",
    "                break\n",
    "        x_vals.append(current_point_x),y_vals.append(current_point_y)\n",
    "        z_vals.append(func(current_point_x,current_point_y))\n",
    "        \n",
    "    return min(z_vals)\n",
    "        \n",
    "    \n",
    "gradient_descent_for_2variables(f3, df3_dx,df3_dy,xlim3,ylim3, 0.001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7336ff0",
   "metadata": {},
   "source": [
    "## Problem 3 - 2-D function \n",
    "\n",
    "Derivatives and limits given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a08ed29e-d92d-4f8d-8785-e9b804dadd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999999046567"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlim4 = [-np.pi, np.pi]\n",
    "ylim4 = [-np.pi,np.pi]\n",
    "def f4(x,y):\n",
    "    return np.exp(-(x - y)**2)*np.sin(y)\n",
    "\n",
    "def f4_dx(x, y):\n",
    "    return -2*np.exp(-(x - y)**2)*np.sin(y)*(x - y)\n",
    "\n",
    "def f4_dy(x, y):\n",
    "    return np.exp(-(x - y)**2)*np.cos(y) + 2*np.exp(-(x - y)**2)*np.sin(y)*(x - y)\n",
    "\n",
    "\n",
    "gradient_descent_for_2variables(f4, f4_dx,f4_dy,xlim4,ylim4, 0.1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f357cd",
   "metadata": {},
   "source": [
    "## Problem 4 - 1-D trigonometric\n",
    "\n",
    "Derivative not given.  Optimization range [0, 2*pi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4684cb5-714d-422e-9e4c-2ea02f865951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f5(x):\n",
    "    return np.cos(x)**4 - np.sin(x)**3 - 4*np.sin(x)**2 + np.cos(x) + 1\n",
    "\n",
    "def gradient_descent_for_1var_2(func, d_fun, start_point_range, learning_rate, precesion=1e-6, max_iterations=10000):\n",
    "    st = np.linspace(start_point_range[0],start_point_range[1],100)\n",
    "    function_values = []\n",
    "    point_values = []\n",
    "    for start_point in st:\n",
    "        current_point = start_point\n",
    "        for i in range(max_iterations):\n",
    "            prev_point = current_point\n",
    "            gradient = d_fun(func,current_point)\n",
    "            current_point -= learning_rate * gradient\n",
    "            if abs(current_point - prev_point) < precesion:\n",
    "                break\n",
    "        point_values.append(current_point)\n",
    "        function_values.append(func(current_point))\n",
    "    return min(function_values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "461bad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_fun(foo,x):\n",
    "    h = 1e-6\n",
    "    der_foo = (foo(x) - foo(x-h))/h\n",
    "    return der_foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "592d86a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.045412051571511"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_for_1var_2(f5, d_fun, [0,2*np.pi], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db403bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multivar(func, derivative, start_point_range, learning_rate, error=1e-6, max_iterations=100000):\n",
    "    for start_point in st:\n",
    "        current_point = start_point\n",
    "        for i in range(max_iterations):\n",
    "            prev_point = current_point\n",
    "            gradient = derivative(current_point)\n",
    "    #         current_point -= learning_rate * gradient\n",
    "            current_point = current_point - learning_rate * gradient\n",
    "            if np.linalg.norm(current_point - prev_point) < error:\n",
    "                break\n",
    "    return func(current_point),current_point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
